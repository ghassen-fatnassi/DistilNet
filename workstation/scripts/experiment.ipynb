{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "device=\"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2)\n",
    "        self.conv = conv_block(2 * out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class segUnet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, depth=5, start_filts=64):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.start_filts = start_filts\n",
    "        self.depth = depth\n",
    "\n",
    "        \"\"\" Encoders \"\"\"\n",
    "        self.encoders = nn.ModuleList([encoder_block(in_channels, start_filts)])\n",
    "        self.encoders.extend([encoder_block(start_filts * (2 ** i), start_filts * (2 ** (i + 1))) for i in range(depth - 1)])\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.bottleneck = conv_block(start_filts * (2 ** (depth - 1)), start_filts * (2 ** depth))\n",
    "\n",
    "        \"\"\" Decoders \"\"\"\n",
    "        self.decoders = nn.ModuleList([decoder_block(start_filts * (2 ** i), start_filts * (2 ** (i - 1))) for i in range(depth, 0, -1)])\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(start_filts, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        skips = []\n",
    "        x = inputs\n",
    "        for encoder in self.encoders:\n",
    "            x, p = encoder(x)\n",
    "            skips.append(x)\n",
    "            x = p\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            x = decoder(x, skips[-(i+1)])\n",
    "\n",
    "        outputs = self.outputs(x)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "segUnet                                  [64, 2, 64, 64]           --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─encoder_block: 2-1                [64, 64, 64, 64]          --\n",
       "│    │    └─conv_block: 3-1              [64, 64, 64, 64]          38,976\n",
       "│    │    └─MaxPool2d: 3-2               [64, 64, 32, 32]          --\n",
       "│    └─encoder_block: 2-2                [64, 128, 32, 32]         --\n",
       "│    │    └─conv_block: 3-3              [64, 128, 32, 32]         221,952\n",
       "│    │    └─MaxPool2d: 3-4               [64, 128, 16, 16]         --\n",
       "│    └─encoder_block: 2-3                [64, 256, 16, 16]         --\n",
       "│    │    └─conv_block: 3-5              [64, 256, 16, 16]         886,272\n",
       "│    │    └─MaxPool2d: 3-6               [64, 256, 8, 8]           --\n",
       "│    └─encoder_block: 2-4                [64, 512, 8, 8]           --\n",
       "│    │    └─conv_block: 3-7              [64, 512, 8, 8]           3,542,016\n",
       "│    │    └─MaxPool2d: 3-8               [64, 512, 4, 4]           --\n",
       "│    └─encoder_block: 2-5                [64, 1024, 4, 4]          --\n",
       "│    │    └─conv_block: 3-9              [64, 1024, 4, 4]          14,161,920\n",
       "│    │    └─MaxPool2d: 3-10              [64, 1024, 2, 2]          --\n",
       "├─conv_block: 1-2                        [64, 2048, 2, 2]          --\n",
       "│    └─Conv2d: 2-6                       [64, 2048, 2, 2]          18,876,416\n",
       "│    └─BatchNorm2d: 2-7                  [64, 2048, 2, 2]          4,096\n",
       "│    └─ReLU: 2-8                         [64, 2048, 2, 2]          --\n",
       "│    └─Conv2d: 2-9                       [64, 2048, 2, 2]          37,750,784\n",
       "│    └─BatchNorm2d: 2-10                 [64, 2048, 2, 2]          4,096\n",
       "│    └─ReLU: 2-11                        [64, 2048, 2, 2]          --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─decoder_block: 2-12               [64, 1024, 4, 4]          --\n",
       "│    │    └─ConvTranspose2d: 3-11        [64, 1024, 4, 4]          8,389,632\n",
       "│    │    └─conv_block: 3-12             [64, 1024, 4, 4]          28,317,696\n",
       "│    └─decoder_block: 2-13               [64, 512, 8, 8]           --\n",
       "│    │    └─ConvTranspose2d: 3-13        [64, 512, 8, 8]           2,097,664\n",
       "│    │    └─conv_block: 3-14             [64, 512, 8, 8]           7,080,960\n",
       "│    └─decoder_block: 2-14               [64, 256, 16, 16]         --\n",
       "│    │    └─ConvTranspose2d: 3-15        [64, 256, 16, 16]         524,544\n",
       "│    │    └─conv_block: 3-16             [64, 256, 16, 16]         1,771,008\n",
       "│    └─decoder_block: 2-15               [64, 128, 32, 32]         --\n",
       "│    │    └─ConvTranspose2d: 3-17        [64, 128, 32, 32]         131,200\n",
       "│    │    └─conv_block: 3-18             [64, 128, 32, 32]         443,136\n",
       "│    └─decoder_block: 2-16               [64, 64, 64, 64]          --\n",
       "│    │    └─ConvTranspose2d: 3-19        [64, 64, 64, 64]          32,832\n",
       "│    │    └─conv_block: 3-20             [64, 64, 64, 64]          110,976\n",
       "├─Conv2d: 1-4                            [64, 2, 64, 64]           130\n",
       "==========================================================================================\n",
       "Total params: 124,386,306\n",
       "Trainable params: 124,386,306\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 270.70\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 2361.39\n",
       "Params size (MB): 497.55\n",
       "Estimated Total Size (MB): 2862.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate the model and move it to the appropriate device\n",
    "model = segUnet(num_classes=2, in_channels=3, depth=5, start_filts=64).to(device)\n",
    "\n",
    "# Print model summary using torchinfo\n",
    "summary(model, input_size=(64, 3, 64, 64), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders: ModuleList\n",
      "  0: encoder_block\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "    pool: MaxPool2d\n",
      "  1: encoder_block\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "    pool: MaxPool2d\n",
      "  2: encoder_block\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "    pool: MaxPool2d\n",
      "  3: encoder_block\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "    pool: MaxPool2d\n",
      "  4: encoder_block\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "    pool: MaxPool2d\n",
      "bottleneck: conv_block\n",
      "  conv1: Conv2d\n",
      "  bn1: BatchNorm2d\n",
      "  conv2: Conv2d\n",
      "  bn2: BatchNorm2d\n",
      "  relu: ReLU\n",
      "decoders: ModuleList\n",
      "  0: decoder_block\n",
      "    up: ConvTranspose2d\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "  1: decoder_block\n",
      "    up: ConvTranspose2d\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "  2: decoder_block\n",
      "    up: ConvTranspose2d\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "  3: decoder_block\n",
      "    up: ConvTranspose2d\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "  4: decoder_block\n",
      "    up: ConvTranspose2d\n",
      "    conv: conv_block\n",
      "      conv1: Conv2d\n",
      "      bn1: BatchNorm2d\n",
      "      conv2: Conv2d\n",
      "      bn2: BatchNorm2d\n",
      "      relu: ReLU\n",
      "outputs: Conv2d\n"
     ]
    }
   ],
   "source": [
    "# Function to recursively print the model's architecture\n",
    "def print_model_structure(model, indent=0):\n",
    "    for name, module in model.named_children():\n",
    "        print('  ' * indent + f'{name}: {module.__class__.__name__}')\n",
    "        print_model_structure(module, indent + 1)\n",
    "\n",
    "# Print the model architecture\n",
    "print_model_structure(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icreatemodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
